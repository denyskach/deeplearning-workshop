{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is a modified version of the [Gluon-NLP Language Model Tutorial](http://gluonnlp.mxnet.io/examples/language_model/language_model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install spacy -U --quiet \n",
    "pip install nltk==3.2.5 -U --quiet\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('perluniprops')\n",
    "nltk.download('nonbreaking_prefixes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.utils import download\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "context = [mx.gpu(0)]\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20 * len(context)\n",
    "lr = 20\n",
    "epochs=3\n",
    "seq_len = 35\n",
    "grad_clip = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Shakespeare Works\n",
    "We will use all the works of shakespeare that is concatenated by Andrej Karpathy, [here](http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt). We took 50% of the dataset and split it into train, val and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./data\"\n",
    "data_url = 'https://s3.amazonaws.com/odsc-conf/shakespeare.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./data/shakespeare.zip from https://s3.amazonaws.com/odsc-conf/shakespeare.zip...\n"
     ]
    }
   ],
   "source": [
    "data_zip = download(data_url, path=data_path)\n",
    "with zipfile.ZipFile(data_zip, 'r') as zipped_data:\n",
    "    zipped_data.extractall(os.path.expanduser(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mimdb\u001b[0m/                 shakespeare_train.txt  \u001b[01;34msherlock\u001b[0m/\r\n",
      "\u001b[01;34m__MACOSX\u001b[0m/             shakespeare_val.txt    \u001b[01;31msherlock.zip\u001b[0m\r\n",
      "shakespeare_test.txt  \u001b[01;31mshakespeare.zip\u001b[0m        timemachine.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_data = [data_path + \"/shakespeare_train.txt\", \n",
    "                 data_path + \"/shakespeare_val.txt\",\n",
    "                data_path + \"/shakespeare_test.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(shakespeare_data[0]) as f:\n",
    "    text = f.read()\n",
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a text splitter that will break our corpus of text into sequences or samples, we will use sentence splitter provided by the [nltk package](https://www.nltk.org/)\n",
    "and  \n",
    "\n",
    "We will use the default tokenizer provided by Gluon-NLP to  split our sequences into words\n",
    "\n",
    "The input to our model are words of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.data.NLTKMosesTokenizer()\n",
    "\n",
    "splitter=nltk.tokenize.sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gluon-NLP provides a **[CorpusDataset](https://gluon-nlp.mxnet.io/api/data.html#gluonnlp.data.CorpusDataset)** API that takes a corpus of text, the splitter and tokenizer functions and creates a dataset object for you, the dataset object can be fed the dataloader APIs to get batches of data(more on it below..).  \n",
    "\n",
    "We will create datasets for all **train, validation and test data** we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds  = [nlp.data.CorpusDataset\n",
    "    (ds, sample_splitter=splitter,flatten=True,eos='<eos>') \n",
    "    for ds in shakespeare_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a Vocabulary from our training dataset. To create a Vocabulary all we need to do is create Counter from our training dataset, which creates a map of **word  : count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = nlp.data.Counter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common tokens: [('<eos>', 20955), ('the', 8600), ('I', 7648), ('to', 6151), ('and', 6044)]\n",
      "\n",
      "unique tokens: 34962\n"
     ]
    }
   ],
   "source": [
    "print(\"5 most common tokens: %s\\n\" % counter.most_common(5))\n",
    "print(\"unique tokens: %s\" % len(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(counter, padding_token=None, bos_token=None, eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=34963, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchify\n",
    "\n",
    "Now we will need to create mini-batches from the sequences of data we have. \n",
    "Gluon-NLP provides batchify function for a given sequence_length and batch_size.   \n",
    "\n",
    "The batchify function creates batches so that the states of the previous batch connects to the hidden state of the current batch.\n",
    "\n",
    "we will use the batchify function in the data loader that feeds the model Training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchify = nlp.data.batchify.CorpusBPTTBatchify(vocab, \n",
    "                       seq_len, batch_size, last_batch='discard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = [batchify(ds) for ds in [train_ds, val_ds, test_ds]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(34963 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(200 -> 34963, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=34963, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'standard_lstm_lm_200'\n",
    "dataset_name='wikitext-2'\n",
    "model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=None, ctx=context[0])\n",
    "print(model)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {\n",
    "    'learning_rate': lr,\n",
    "    'momentum': 0,\n",
    "    'wd': 0\n",
    "})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a slightly modified Backprogation through time algorithm called **Truncated Backpropagtion through time(TBPTT)**, here we will truncate the BPTT algorithm after **k** steps and update the weights, since a long sequence is expensive to compute complete BPTT and also potentially result in Vanishing gradients. \n",
    "\n",
    "We truncate by detaching the hidden state after **k** steps. Let's write a method for detaching the hidden state.\n",
    "\n",
    "Reference:  \n",
    "1. [Understanding BPTT & TBPTT conceptutally](https://machinelearningmastery.com/gentle-introduction-backpropagation-time/)  \n",
    "\n",
    "2. [BPTT & TBPTT in detail](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html?highlight=detach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a evaluate method that will use the model on a dataset and measure the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = detach(hidden)\n",
    "        \n",
    "        L = loss(output.reshape(-3, -1), target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        \n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx)\n",
    "                   for ctx in context]\n",
    "        \n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context, \n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context, \n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            \n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output.reshape(-3, -1), y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / (len(context) * X.size)\n",
    "                    Ls.append(batch_L / (len(context) * X.size))\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            \n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L),\n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "        val_L = evaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, batch_size, context[0])\n",
    "            model.save_parameters('{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) /\n",
    "                            (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the performance ofthe pre-trained model we fetched from Gluon-NLP without training using the shakespeare  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss 10.46, test ppl 34962.08\n"
     ]
    }
   ],
   "source": [
    "shakespeare_L = evaluate(model, test_dl, batch_size, context[0])\n",
    "print('Best validation loss %.2f, test ppl %.2f' %\n",
    "      (shakespeare_L, math.exp(shakespeare_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 200/499] loss 6.99, ppl 1090.20, throughput 480.35 samples/s\n",
      "[Epoch 0 Batch 400/499] loss 6.68, ppl 799.54, throughput 478.89 samples/s\n",
      "[Epoch 0] throughput 476.66 samples/s\n",
      "[Epoch 0] time cost 26.42s, valid loss 6.84, valid ppl 934.93\n",
      "test loss 6.84, test ppl 934.93\n",
      "[Epoch 1 Batch 200/499] loss 6.39, ppl 595.78, throughput 476.75 samples/s\n",
      "[Epoch 1 Batch 400/499] loss 6.27, ppl 526.18, throughput 476.46 samples/s\n",
      "[Epoch 1] throughput 479.42 samples/s\n",
      "[Epoch 1] time cost 26.30s, valid loss 6.68, valid ppl 797.85\n",
      "test loss 6.68, test ppl 797.85\n",
      "[Epoch 2 Batch 200/499] loss 6.10, ppl 444.39, throughput 483.13 samples/s\n",
      "[Epoch 2 Batch 400/499] loss 6.02, ppl 410.57, throughput 474.56 samples/s\n",
      "[Epoch 2] throughput 481.56 samples/s\n",
      "[Epoch 2] time cost 26.20s, valid loss 6.66, valid ppl 778.30\n",
      "test loss 6.66, test ppl 778.30\n",
      "[Epoch 3 Batch 200/499] loss 5.90, ppl 364.99, throughput 482.56 samples/s\n",
      "[Epoch 3 Batch 400/499] loss 5.84, ppl 343.06, throughput 464.88 samples/s\n",
      "[Epoch 3] throughput 477.66 samples/s\n",
      "[Epoch 3] time cost 26.39s, valid loss 6.64, valid ppl 761.88\n",
      "test loss 6.64, test ppl 761.88\n",
      "[Epoch 4 Batch 200/499] loss 5.73, ppl 307.99, throughput 479.15 samples/s\n",
      "[Epoch 4 Batch 400/499] loss 5.67, ppl 291.48, throughput 464.80 samples/s\n",
      "[Epoch 4] throughput 472.38 samples/s\n",
      "[Epoch 4] time cost 26.60s, valid loss 6.65, valid ppl 775.62\n",
      "Learning rate now 0.025000\n",
      "[Epoch 5 Batch 200/499] loss 5.65, ppl 285.49, throughput 481.20 samples/s\n",
      "[Epoch 5 Batch 400/499] loss 5.53, ppl 252.95, throughput 475.78 samples/s\n",
      "[Epoch 5] throughput 480.34 samples/s\n",
      "[Epoch 5] time cost 26.24s, valid loss 6.64, valid ppl 763.92\n",
      "Learning rate now 0.006250\n",
      "[Epoch 6 Batch 200/499] loss 5.63, ppl 279.06, throughput 473.41 samples/s\n",
      "[Epoch 6 Batch 400/499] loss 5.52, ppl 249.88, throughput 472.63 samples/s\n",
      "[Epoch 6] throughput 471.69 samples/s\n",
      "[Epoch 6] time cost 26.60s, valid loss 6.64, valid ppl 762.74\n",
      "Learning rate now 0.001563\n",
      "[Epoch 7 Batch 200/499] loss 5.63, ppl 278.80, throughput 463.97 samples/s\n",
      "[Epoch 7 Batch 400/499] loss 5.52, ppl 248.71, throughput 489.81 samples/s\n",
      "[Epoch 7] throughput 475.66 samples/s\n",
      "[Epoch 7] time cost 26.48s, valid loss 6.64, valid ppl 762.48\n",
      "Learning rate now 0.000391\n",
      "[Epoch 8 Batch 200/499] loss 5.63, ppl 277.62, throughput 478.03 samples/s\n",
      "[Epoch 8 Batch 400/499] loss 5.52, ppl 248.68, throughput 458.78 samples/s\n",
      "[Epoch 8] throughput 473.94 samples/s\n",
      "[Epoch 8] time cost 26.56s, valid loss 6.64, valid ppl 762.42\n",
      "Learning rate now 0.000098\n",
      "[Epoch 9 Batch 200/499] loss 5.63, ppl 277.95, throughput 482.97 samples/s\n",
      "[Epoch 9 Batch 400/499] loss 5.52, ppl 249.65, throughput 457.57 samples/s\n",
      "[Epoch 9] throughput 472.84 samples/s\n",
      "[Epoch 9] time cost 26.64s, valid loss 6.64, valid ppl 762.41\n",
      "Learning rate now 0.000024\n",
      "Total training throughput 347.65 samples/s\n"
     ]
    }
   ],
   "source": [
    "train( model,train_dl,val_dl,test_dl,epochs=10, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "Lets see if our model can now generate text like shaekspeare :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"boston\"]\n",
    "input = mx.nd.array([vocab[sentence[0]]], ctx=context[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden = model.begin_state(batch_size=1, ctx=context[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,text_len):\n",
    "    input = mx.nd.expand_dims(input, axis=1) # (batch_size * seq)\n",
    "    output, hidden = model(input, hidden)\n",
    "    output = mx.nd.argmax(output[0], axis=1)\n",
    "    input = output\n",
    "    sentence.append(vocab.idx_to_token[output[0].astype(\"int\").asscalar()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = []\n",
    "for each in sentence:\n",
    "    if each != '<eos>':\n",
    "        line.append(each)\n",
    "    else:\n",
    "        sentences.append(line)\n",
    "        line=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston The king is in the world of my life, And I will tell thee what I do not see The king of my poor heart to the king.\n",
      "CAPULET: I pray you, sir, I will not be a king, And I will tell you what I do not see The king of my good heart to the king.\n",
      "KING RICHARD III: Well, I will not be a man to you.\n",
      "QUEEN MARGARET: I will not be a king, I will not be a king.\n",
      "QUEEN MARGARET: I will not be a king, I will not be a king.\n",
      "QUEEN MARGARET: I will not be a gentleman to hear the king.\n",
      "QUEEN MARGARET: I will not be a gentleman to hear the king to the king.\n",
      "KING RICHARD III: Well, I will not be a man to hear the king.\n",
      "QUEEN MARGARET: I will not be a gentleman to be a man.\n",
      "ROMEO: I pray you, sir, I am a gentleman of my heart.\n"
     ]
    }
   ],
   "source": [
    "for each in sentences:\n",
    "    print(' '.join(each))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
