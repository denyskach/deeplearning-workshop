{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is a modified version of the [Gluon-NLP Language Model Tutorial](http://gluonnlp.mxnet.io/examples/language_model/language_model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/en_core_web_sm\n",
      "-->\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mxnet-cu90mkl 1.4.0.post0 has requirement numpy<1.15.0,>=1.8.2, but you'll have numpy 1.16.3 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 10.0.1, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "/home/ec2-user/anaconda3/envs/mxnet_p36/bin/pip install nltk==3.2.5 -U --quiet\n",
    "sudo /home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('perluniprops')\n",
    "nltk.download('nonbreaking_prefixes')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.test_utils import download\n",
    "\n",
    "import gluonnlp as nlp\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "context = [mx.gpu(0)]\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20 * len(context)\n",
    "lr = 20\n",
    "epochs=3\n",
    "seq_len = 35\n",
    "grad_clip = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Shakespeare Works\n",
    "We will use all the works of shakespeare that is concatenated by Andrej Karpathy, [here](http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt). We took 50% of the dataset and split it into train, val and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./data\"\n",
    "data_url = 'https://s3.amazonaws.com/odsc-conf/shakespeare.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_zip = download(data_url,dirname=data_path)\n",
    "with zipfile.ZipFile(data_zip, 'r') as zipped_data:\n",
    "    zipped_data.extractall(os.path.expanduser(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shakespeare_test.txt  shakespeare_train.txt  shakespeare_val.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls \"./data/shakespeare/\"\n",
    "data_path = \"./data/shakespeare/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_data = [data_path + \"/shakespeare_train.txt\", \n",
    "                 data_path + \"/shakespeare_val.txt\",\n",
    "                data_path + \"/shakespeare_test.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open(shakespeare_data[0]) as f:\n",
    "    text = f.read()\n",
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a text splitter that will break our corpus of text into sequences or samples, we will use sentence splitter provided by the [nltk package](https://www.nltk.org/)\n",
    "and  \n",
    "\n",
    "We will use the default tokenizer provided by Gluon-NLP to  split our sequences into words\n",
    "\n",
    "The input to our model are words of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.data.NLTKMosesTokenizer()\n",
    "\n",
    "splitter=nltk.tokenize.sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gluon-NLP provides a **[CorpusDataset](https://gluon-nlp.mxnet.io/api/data.html#gluonnlp.data.CorpusDataset)** API that takes a corpus of text, the splitter and tokenizer functions and creates a dataset object for you, the dataset object can be fed the dataloader APIs to get batches of data(more on it below..).  \n",
    "\n",
    "We will create datasets for all **train, validation and test data** we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds  = [nlp.data.CorpusDataset\n",
    "    (ds, sample_splitter=splitter,flatten=True,eos='<eos>') \n",
    "    for ds in shakespeare_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a Vocabulary from our training dataset. To create a Vocabulary all we need to do is create Counter from our training dataset, which creates a map of **word  : count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = nlp.data.Counter(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common tokens: [('<eos>', 20955), ('the', 8600), ('I', 7648), ('to', 6151), ('and', 6044)]\n",
      "\n",
      "unique tokens: 34962\n"
     ]
    }
   ],
   "source": [
    "print(\"5 most common tokens: %s\\n\" % counter.most_common(5))\n",
    "print(\"unique tokens: %s\" % len(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.Vocab(counter, padding_token=None, bos_token=None, eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=34963, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchify\n",
    "\n",
    "Now we will need to create mini-batches from the sequences of data we have. \n",
    "Gluon-NLP provides batchify function for a given sequence_length and batch_size.   \n",
    "\n",
    "The batchify function creates batches so that the states of the previous batch connects to the hidden state of the current batch.\n",
    "\n",
    "we will use the batchify function in the data loader that feeds the model Training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchify = nlp.data.batchify.CorpusBPTTBatchify(vocab, \n",
    "                       seq_len, batch_size, last_batch='discard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = [batchify(ds) for ds in [train_ds, val_ds, test_ds]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(34963 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(200 -> 34963, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=34963, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'standard_lstm_lm_200'\n",
    "dataset_name='wikitext-2'\n",
    "model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=None, ctx=context[0])\n",
    "print(model)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {\n",
    "    'learning_rate': lr,\n",
    "    'momentum': 0,\n",
    "    'wd': 0\n",
    "})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a slightly modified Backprogation through time algorithm called **Truncated Backpropagtion through time(TBPTT)**, here we will truncate the BPTT algorithm after **k** steps and update the weights, since a long sequence is expensive to compute complete BPTT and also potentially result in Vanishing gradients. \n",
    "\n",
    "We truncate by detaching the hidden state after **k** steps. Let's write a method for detaching the hidden state.\n",
    "\n",
    "Reference:  \n",
    "1. [Understanding BPTT & TBPTT conceptutally](https://machinelearningmastery.com/gentle-introduction-backpropagation-time/)  \n",
    "\n",
    "2. [BPTT & TBPTT in detail](https://d2l.ai/chapter_recurrent-neural-networks/bptt.html?highlight=detach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a evaluate method that will use the model on a dataset and measure the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = detach(hidden)\n",
    "        \n",
    "        L = loss(output.reshape(-3, -1), target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        \n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx)\n",
    "                   for ctx in context]\n",
    "        \n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context, \n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context, \n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            \n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output.reshape(-3, -1), y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / (len(context) * X.size)\n",
    "                    Ls.append(batch_L / (len(context) * X.size))\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            \n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L),\n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "        val_L = evaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, batch_size, context[0])\n",
    "            model.save_parameters('{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) /\n",
    "                            (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the performance ofthe pre-trained model we fetched from Gluon-NLP without training using the shakespeare  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss 10.46, test ppl 34962.08\n"
     ]
    }
   ],
   "source": [
    "shakespeare_L = evaluate(model, test_dl, batch_size, context[0])\n",
    "print('Best validation loss %.2f, test ppl %.2f' %\n",
    "      (shakespeare_L, math.exp(shakespeare_L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 200/499] loss 8.21, ppl 3666.79, throughput 461.63 samples/s\n",
      "[Epoch 0 Batch 400/499] loss 7.19, ppl 1328.29, throughput 473.86 samples/s\n",
      "[Epoch 0] throughput 472.51 samples/s\n",
      "[Epoch 0] time cost 26.59s, valid loss 7.04, valid ppl 1146.89\n",
      "test loss 7.04, test ppl 1146.89\n",
      "[Epoch 1 Batch 200/499] loss 6.64, ppl 768.85, throughput 475.40 samples/s\n",
      "[Epoch 1 Batch 400/499] loss 6.45, ppl 634.75, throughput 474.67 samples/s\n",
      "[Epoch 1] throughput 472.25 samples/s\n",
      "[Epoch 1] time cost 26.27s, valid loss 6.75, valid ppl 852.01\n",
      "test loss 6.75, test ppl 852.01\n",
      "[Epoch 2 Batch 200/499] loss 6.24, ppl 515.15, throughput 475.81 samples/s\n",
      "[Epoch 2 Batch 400/499] loss 6.13, ppl 460.66, throughput 482.63 samples/s\n",
      "[Epoch 2] throughput 479.49 samples/s\n",
      "[Epoch 2] time cost 26.32s, valid loss 6.66, valid ppl 781.14\n",
      "test loss 6.66, test ppl 781.14\n",
      "[Epoch 3 Batch 200/499] loss 6.00, ppl 404.99, throughput 474.69 samples/s\n",
      "[Epoch 3 Batch 400/499] loss 5.92, ppl 373.72, throughput 477.44 samples/s\n",
      "[Epoch 3] throughput 471.69 samples/s\n",
      "[Epoch 3] time cost 26.61s, valid loss 6.65, valid ppl 774.46\n",
      "test loss 6.65, test ppl 774.46\n",
      "[Epoch 4 Batch 200/499] loss 5.82, ppl 337.43, throughput 484.85 samples/s\n",
      "[Epoch 4 Batch 400/499] loss 5.75, ppl 315.04, throughput 464.94 samples/s\n",
      "[Epoch 4] throughput 479.45 samples/s\n",
      "[Epoch 4] time cost 26.24s, valid loss 6.67, valid ppl 788.77\n",
      "Learning rate now 0.025000\n",
      "[Epoch 5 Batch 200/499] loss 5.73, ppl 307.08, throughput 459.14 samples/s\n",
      "[Epoch 5 Batch 400/499] loss 5.61, ppl 272.87, throughput 471.35 samples/s\n",
      "[Epoch 5] throughput 470.14 samples/s\n",
      "[Epoch 5] time cost 26.70s, valid loss 6.65, valid ppl 773.57\n",
      "test loss 6.65, test ppl 773.57\n",
      "[Epoch 6 Batch 200/499] loss 5.70, ppl 299.88, throughput 473.83 samples/s\n",
      "[Epoch 6 Batch 400/499] loss 5.59, ppl 266.84, throughput 489.83 samples/s\n",
      "[Epoch 6] throughput 477.58 samples/s\n",
      "[Epoch 6] time cost 26.36s, valid loss 6.64, valid ppl 767.72\n",
      "test loss 6.64, test ppl 767.72\n",
      "[Epoch 7 Batch 200/499] loss 5.69, ppl 295.17, throughput 479.95 samples/s\n",
      "[Epoch 7 Batch 400/499] loss 5.57, ppl 263.60, throughput 479.30 samples/s\n",
      "[Epoch 7] throughput 477.58 samples/s\n",
      "[Epoch 7] time cost 26.37s, valid loss 6.64, valid ppl 764.38\n",
      "test loss 6.64, test ppl 764.38\n",
      "[Epoch 8 Batch 200/499] loss 5.68, ppl 291.86, throughput 481.66 samples/s\n",
      "[Epoch 8 Batch 400/499] loss 5.56, ppl 260.34, throughput 465.26 samples/s\n",
      "[Epoch 8] throughput 478.48 samples/s\n",
      "[Epoch 8] time cost 26.32s, valid loss 6.64, valid ppl 761.86\n",
      "test loss 6.64, test ppl 761.86\n",
      "[Epoch 9 Batch 200/499] loss 5.67, ppl 289.09, throughput 480.00 samples/s\n",
      "[Epoch 9 Batch 400/499] loss 5.55, ppl 258.48, throughput 474.53 samples/s\n",
      "[Epoch 9] throughput 481.41 samples/s\n",
      "[Epoch 9] time cost 26.20s, valid loss 6.63, valid ppl 759.87\n",
      "test loss 6.63, test ppl 759.87\n",
      "Total training throughput 317.19 samples/s\n"
     ]
    }
   ],
   "source": [
    "train( model,train_dl,val_dl,test_dl,epochs=10, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "Lets see if our model can now generate text like shaekspeare :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"boston\"]\n",
    "input = mx.nd.array([vocab[sentence[0]]], ctx=context[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden = model.begin_state(batch_size=1, ctx=context[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,text_len):\n",
    "    input = mx.nd.expand_dims(input, axis=1) # (batch_size * seq)\n",
    "    output, hidden = model(input, hidden)\n",
    "    output = mx.nd.argmax(output[0], axis=1)\n",
    "    input = output\n",
    "    sentence.append(vocab.idx_to_token[output[0].astype(\"int\").asscalar()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston and the world is not a great man, and the poor man is a great man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a man to be a\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
