{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will build a LSTM Model that can classify a text sentence as expressing either positive sentiment or negative sentiment.\n",
    "\n",
    "Note: This notebook has been borrowed from [GLUON-NLP Tutorials](https://gluon-nlp.mxnet.io/examples/sentiment_analysis/sentiment_analysis.html) and enhanced for this workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm: started\n",
      "    Running setup.py install for en-core-web-sm: finished with status 'done'\n",
      "Successfully installed en-core-web-sm-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/en_core_web_sm\n",
      "-->\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mxnet-cu90mkl 1.4.0.post0 has requirement numpy<1.15.0,>=1.8.2, but you'll have numpy 1.16.3 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 10.0.1, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Install nltk\n",
    "/home/ec2-user/anaconda3/envs/mxnet_p36/bin/pip install nltk==3.2.5 -U --quiet\n",
    "\n",
    "# Download Spacy resources for English Language Model\n",
    "sudo /home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the usual: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "CPU_COUNT = cpu_count()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Context in which to train the model. \n",
    "# Use mx.cpu() for CPU and mx.gpu(0), 0 is the index of GPU - we have 1 GPU on this instance, so we will set \n",
    "# context to mx.gpu(0)\n",
    "\n",
    "context = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset: IMDb\n",
    "To build the model We will use Stanford's Large Movie Review Dataset from IMDb as the data set for sentiment classification[1]. \n",
    "This data set is divided into two data sets for training and testing purposes, \n",
    "each containing 25,000 movie reviews downloaded from IMDb. \n",
    "\n",
    "In each data set, the number of comments labeled as \"positive\" and \"negative\" is equal.\n",
    "\n",
    "**[Gluon-NLP](http://gluon-nlp.mxnet.io/)** provides many of the standard benchmark datasets used for research, we will download both the `train` and `test` IMDb datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the train and test IMDB movie review dataset from gluon-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data/imdb/train.json from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/imdb/train.json...\n",
      "Downloading data/imdb/test.json from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/imdb/test.json...\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = [nlp.data.IMDB(root='data/imdb', segment=segment)\n",
    "                               for segment in ('train', 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.json  train.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"data/imdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \\\"Teachers\\\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \\\"Teachers\\\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\", 9], [\"Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everythin"
     ]
    }
   ],
   "source": [
    "! head -c 1000 \"data/imdb/train.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model, Embedding and Vocabulary from Gluon-NLP\n",
    "Before we proceed further to process our dataset, lets firt lets download the Vocabulary, Embedding and the model to use in our pre-processing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /home/ec2-user/.mxnet/models/wikitext-2-be36dc52.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/wikitext-2-be36dc52.zip...\n",
      "Downloading /home/ec2-user/.mxnet/models/standard_lstm_lm_200_wikitext-2-b233c700.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/standard_lstm_lm_200_wikitext-2-b233c700.zip...\n"
     ]
    }
   ],
   "source": [
    "model, vocab = nlp.model.get_model(name='standard_lstm_lm_200',\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=True,\n",
    "                                      ctx=context,\n",
    "                                      dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardRNN(\n",
       "  (embedding): HybridSequential(\n",
       "    (0): Embedding(33278 -> 200, float32)\n",
       "  )\n",
       "  (encoder): LSTM(200 -> 200, TNC, num_layers=2)\n",
       "  (decoder): HybridSequential(\n",
       "    (0): Dense(200 -> 33278, linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tokenize Sentences Into words\n",
    "\n",
    "We use the Spacy tokenizer to split the sentences into words which are the tokens for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.data.SpacyTokenizer('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Clip the input sample(each review)\n",
    "\n",
    "Since it needs a whole lot of memory to learn long sentences, we will clip the review to at most 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_clip = nlp.data.ClipSequence(seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Preprocessing\n",
    "\n",
    "We need to do some pre-processing of the input data, specifically we want to categorize our labels which have a review score of > 5 as positive and <=5 as negative ie., our `label=1` else `label=0` \n",
    "\n",
    "We will also convert our tokenized sequence using a vocabulary to indexes that can be fed to the network. The below routine will do the preprocessing job on each data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    data, label = x\n",
    "    label = int(label > 5)\n",
    "    # Tokenize the data\n",
    "    tokenized_data = tokenizer(data)\n",
    "    # Clip the tokens\n",
    "    tokenized_clipped_data = length_clip(tokenized_data)\n",
    "    # Get vocabulary indexes for the tokens. Use pre-loaded 'vocab'.\n",
    "    data = vocab[tokenized_clipped_data]\n",
    "\n",
    "    return data, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def preprocess_dataset(dataset, vocab):\n",
    "    \n",
    "    with mp.Pool(CPU_COUNT) as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    \n",
    "    return dataset, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. #### Preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Train dataset. This will take few minutes...\n",
      "Preparing Test dataset. This will take few minutes...\n",
      "Data is ready!!!\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset\n",
    "print(\"Preparing Train dataset. This will take few minutes...\")\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset, vocab)\n",
    "\n",
    "print(\"Preparing Test dataset. This will take few minutes...\")\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset, vocab)\n",
    "\n",
    "print(\"Data is ready!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. #### Batchify, Samplers and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 Batchify\n",
    "we will need to create mini-batches from the sequences of data we have. Gluon-NLP provides batchify function for a given sequence_length and batch_size.   \n",
    "\n",
    "The batchify function creates batches so that the states of the previous batch connects to the hidden state of the current batch.\n",
    "\n",
    "we will use the batchify function in the data loader that feeds the model Training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(axis=0, ret_length=True), \n",
    "    nlp.data.batchify.Stack(dtype='float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2 Sampler\n",
    "Since the inputs are of different lengths(not necessarily all are > seq_len), \n",
    "its not ideal to pad empty on shorter sequences.\n",
    "    \n",
    "we will use FixedBucketSampler which creates multiple buckets(based on a num buckets and a ratio) of different lengths and \n",
    "assigns each data sample to a fixed bucket based on its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bucket_num, bucket_ratio = 10, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=25000, batch_num=779\n",
      "  key=[59, 108, 157, 206, 255, 304, 353, 402, 451, 500]\n",
      "  cnt=[591, 1999, 5092, 5108, 3035, 2084, 1476, 1164, 871, 3580]\n",
      "  batch_size=[54, 32, 32, 32, 32, 32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "print(batch_sampler.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.3 Data Loaders:\n",
    "* Finally we will pad sequences which are not of the bucket lenght \n",
    "* stack data, labels and length of data for each datasample\n",
    "\n",
    "We feed all these methods to the DataLoader object which applies the above preprocess steps every iteration before feeding the input of the network. We will iterate over the data one batch at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = gluon.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_sampler=batch_sampler,\n",
    "    batchify_fn=batchify_fn, num_workers=CPU_COUNT)\n",
    "\n",
    "test_dataloader = gluon.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    batchify_fn=batchify_fn, num_workers=CPU_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model with Pre-trained Model\n",
    "\n",
    "We will using a techique called [Transfer Learning](http://cs231n.github.io/transfer-learning/) to build this model,\n",
    "where we use a pre-trained model (in this case a Language Model), \n",
    "and a pre-trained embedding to represent our input data. \n",
    "\n",
    "The model we use here is a standard [standard_lstm_lm_200](https://gluon-nlp.mxnet.io/api/model.html#gluonnlp.model.standard_lstm_lm_200) \n",
    "\n",
    "This is a 2-layer standard LSTM with 200 hidden units and an embedding size of 200.\n",
    "\n",
    "The model and the Embedding has been trained on the **WikiText-2** dataset which consists of around 2 million words extracted from Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Network\n",
    "\n",
    "**Encoder:** We will use the pre-trained Embedding, the pre-trained Model(which is a language Model) as an encoder.\n",
    "\n",
    "**MeanPool:** We will take the LSTM representation of the LSTM and average the predictions across timestamps (since we don't need output at every step), just at the end of the sequence to know the sentiment).\n",
    "\n",
    "**Output:** The aggregated output is fed to a Dense Layer followed by Softmax to get an probabilty across the 2 classes (positive and negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MeanPoolLayer\n",
    "\n",
    "MeanPoolLayer aggregates output across timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingLayer(gluon.HybridBlock):\n",
    "    \"\"\"A block for mean pooling of encoder features\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(MeanPoolingLayer, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length):\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_state = F.broadcast_div(F.sum(masked_encoded, axis=0),\n",
    "                                    F.expand_dims(valid_length, axis=1))\n",
    "        return agg_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "The Model consists of an Embedding, Encoder (LSTM), MeanPool and a Dense layer\n",
    "\n",
    "![Sentiment Analyzer](network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gluon.nn.Dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = None # will set with lm embedding later\n",
    "            self.encoder = None # will set with lm encoder later\n",
    "            self.agg_layer = MeanPoolingLayer()\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                self.output.add(gluon.nn.Dense(1, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): \n",
    "        embedded = self.embedding(data)\n",
    "        encoded = self.encoder(embedded)\n",
    "        agg_state = self.agg_layer(encoded, valid_length)\n",
    "        out = self.output(agg_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Network with Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2)\n",
      "  (agg_layer): MeanPoolingLayer(\n",
      "  \n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Dense(None -> 1, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SentimentNet()\n",
    "\n",
    "# Use Pretrained Embeddings from wikitext-2\n",
    "net.embedding = model.embedding\n",
    "\n",
    "# Use Pretrained Encoder states (LSTM) from wikitext-2\n",
    "net.encoder = model.encoder\n",
    "\n",
    "#net.hybridize()\n",
    "\n",
    "# Random initialize the last Dense Laywer\n",
    "net.output.initialize(mx.init.Xavier(), ctx=context)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, context):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    for i, ((data, valid_length), label) in enumerate(dataloader):\n",
    "\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        \n",
    "        output = net(data, valid_length)\n",
    "        \n",
    "        L = loss(output, label)\n",
    "        \n",
    "        pred = (output > 0.5).reshape(-1)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    # Use Follow the Moving Leader Optimizer - [7]\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    parameters = net.collect_params().values()\n",
    "    start_train_time = time.time()\n",
    "    print(\"Training the Sentiment Classification Model...\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_L = 0.0\n",
    "        start_epoch_time = time.time()        \n",
    "        epoch_sent_num = 0\n",
    "        for i, ((data, length), label) in enumerate(train_dataloader):\n",
    "            L = 0\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(context).T,\n",
    "                             length.as_in_context(context)\n",
    "                                   .astype(np.float32))\n",
    "\n",
    "                L = L + loss(output, label.as_in_context(context)).mean()\n",
    "            \n",
    "            L.backward()\n",
    "            \n",
    "            trainer.step(1)\n",
    "            \n",
    "            epoch_sent_num += data.shape[1]\n",
    "            epoch_L += L.asscalar()\n",
    "    \n",
    "        print('Train Avg Loss: {:.6f}'.format(epoch_L / epoch_sent_num))\n",
    "        \n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context)\n",
    "        print('Test Accuracy: {:.2f}, Test Avg Loss: {:.6f}'.format(test_acc, test_avg_L))\n",
    "\n",
    "        print('[Epoch {}] time cost: {:.2f}'.format(epoch, time.time()-start_epoch_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Sentiment Classification Model...\n",
      "Train Avg Loss: 0.001514\n",
      "Test Accuracy: 0.87, Test Avg Loss: 0.319959\n",
      "[Epoch 0] time cost: 48.52\n",
      "Train Avg Loss: 0.000690\n",
      "Test Accuracy: 0.86, Test Avg Loss: 0.318548\n",
      "[Epoch 1] time cost: 41.65\n",
      "Train Avg Loss: 0.000277\n",
      "Test Accuracy: 0.85, Test Avg Loss: 0.404589\n",
      "[Epoch 2] time cost: 41.73\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(net, context, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Inventor: Out for Blood in Silicon Valley**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1 = \"I had read John Carreyrou's fine Wall Street Journal articles, \\\n",
    "as well as his thrilling book, Bad Blood, before seeing this documentary tonight. \\\n",
    "The first half of the documentary seems almost worshipful of Elizabeth Holmes, \\\n",
    "building up her mystique and putting her unique ability to attract doting followers \\\n",
    "to her message on display. Quite a lot of time is spent gazing into those big blue, \\\n",
    "unblinking eyes. By the time we get around to the cracks in the facade, \\\n",
    "we are more than an hour into the film. \\\n",
    "It is inevitable that a lot of important background was left out: \\\n",
    "the climate of constant firings that went on for years, the fact that \\\n",
    "Sunny and Elizabeth met when he was 38 (and married) and she was 19, \\\n",
    "that Elizabeth's dad had been a VP at Enron, etc. \\\n",
    "Mostly I would have appreciated a little more specific information on why the Edison machine failed. \\\n",
    "The examples given in the film don't seem that unsolvable, \\\n",
    "but I know from the book that there were some basic issues that simply \\\n",
    "couldn't be dreamed away owing to the tiny sample sizes from the finger pricks. \\\n",
    "Tyler Shultz comes off as a happy-go-lucky guy, but in fact he is one of the heroes of this story. \\\n",
    "It is not mentioned in this film, but not just his grandfather former Secretary of State and Theranos \\\n",
    "board member George Schultz, but also his parents flipped out when he told them he was quitting the company. \\\n",
    "His bravery in standing up for his values is truly admirable in one so young, \\\n",
    "especially considering the immense pressure he came under. To his parents' credit, \\\n",
    "they came around and ended up mortgaging their home to pay his legal bills. \\\n",
    "Ultimately, though, the story gets Elizabeth right: she is a zealot who is deaf to any naysayers, \\\n",
    "even to this day. The cautionary tale for the rest of us, is are we George Shultz or Tyler Shultz?\\\n",
    "Are we willing to see the truth and make a difficult decision, or are we too invested to be willing \\\n",
    "to give up on something we had believed in?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**review2**](https://www.imdb.com/review/rw4739739/?ref_=tt_urv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "review2 = \"The beginning seems to draw a parallel between Holmes and Edison. \\\n",
    "His for lying for four years to investors about incandescent lightbulbs, hers \\\n",
    "for a one drop of blood test. Not accurate. She allowed people to make personal, \\\n",
    "medical decisions, but all he was doing was putting light in people's homes. \\\n",
    "They are completely different and not even in the same sphere.As for the rest, \\\n",
    "I got sick of an almost worship of this selfish woman. \\\n",
    "I agree with another about how major factors that went on at that company were left out of this documentary. \\\n",
    "The fact that her lawyers were threatening, stalking, and tapping her former employees, nearly crossed, \\\n",
    "if not crossed, the line in to harassment. If you want the real story, and not a lot of fluff, \\\n",
    "and an almost hero worship, read 'Bad Blood,' by John Carreyrou.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review1 len:400\n",
      "review2 len:168\n"
     ]
    }
   ],
   "source": [
    "review1 = tokenizer(review1)\n",
    "review2 = tokenizer(review2)\n",
    "print('review1 len:{}'.format(len(review1)))\n",
    "print('review2 len:{}'.format(len(review2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "review1 = [vocab[word] for word in review1]\n",
    "review2 = [vocab[word] for word in review2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment -  positive\n"
     ]
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(review1, ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "print(\"Sentiment - \", 'positive' if prob1[0] > 0.5 else 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment -  negative\n"
     ]
    }
   ],
   "source": [
    "prob2 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(review2, ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([5], ctx=context)).sigmoid()\n",
    "print(\"Sentiment - \", 'positive' if prob2[0] > 0.5 else 'negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check it out yourselves :**    \n",
    "\n",
    "[**review 1**](https://www.imdb.com/review/rw4732229/?ref_=tt_urv)\n",
    "\n",
    "[**review2**](https://www.imdb.com/review/rw4739739/?ref_=tt_urv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
